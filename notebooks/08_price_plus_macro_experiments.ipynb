{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c886c5de",
   "metadata": {},
   "source": [
    "# Notebook 08 – Price + Macro Indicators (Daily SPY)\n",
    "\n",
    "Goal: Augment the daily SPY feature set with macro / market-wide indicators\n",
    "(VIX, yield curve etc.), and evaluate whether they improve next-day direction\n",
    "prediction compared to price-only MLP/GRU baselines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38826ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: C:\\Users\\KDP only\\Documents\\ANN_Final_Project\\spy-ann\n",
      "DATA_PROC: C:\\Users\\KDP only\\Documents\\ANN_Final_Project\\spy-ann\\data\\processed\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import yfinance as yf\n",
    "\n",
    "ROOT = Path(\"..\").resolve()\n",
    "DATA_PROC = ROOT / \"data\" / \"processed\"\n",
    "\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"DATA_PROC:\", DATA_PROC)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe30dbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DAILY SPY (BASE) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "      <th>ret_1d</th>\n",
       "      <th>log_ret_1d</th>\n",
       "      <th>ma_close_5</th>\n",
       "      <th>ma_close_20</th>\n",
       "      <th>vol_5</th>\n",
       "      <th>vol_20</th>\n",
       "      <th>future_price</th>\n",
       "      <th>future_ret_1d</th>\n",
       "      <th>label_up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-02-02</td>\n",
       "      <td>83.059364</td>\n",
       "      <td>83.217386</td>\n",
       "      <td>81.930636</td>\n",
       "      <td>82.216584</td>\n",
       "      <td>216327900</td>\n",
       "      <td>0.012104</td>\n",
       "      <td>0.012031</td>\n",
       "      <td>82.055548</td>\n",
       "      <td>84.347997</td>\n",
       "      <td>0.012653</td>\n",
       "      <td>0.010585</td>\n",
       "      <td>82.645493</td>\n",
       "      <td>-0.004983</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-02-03</td>\n",
       "      <td>82.645493</td>\n",
       "      <td>83.134609</td>\n",
       "      <td>82.404697</td>\n",
       "      <td>82.683113</td>\n",
       "      <td>172730700</td>\n",
       "      <td>-0.004983</td>\n",
       "      <td>-0.004995</td>\n",
       "      <td>82.055548</td>\n",
       "      <td>84.205024</td>\n",
       "      <td>0.012873</td>\n",
       "      <td>0.010574</td>\n",
       "      <td>80.094604</td>\n",
       "      <td>-0.030865</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-02-04</td>\n",
       "      <td>80.094604</td>\n",
       "      <td>82.043540</td>\n",
       "      <td>80.079552</td>\n",
       "      <td>82.005919</td>\n",
       "      <td>356715700</td>\n",
       "      <td>-0.030865</td>\n",
       "      <td>-0.031352</td>\n",
       "      <td>81.734995</td>\n",
       "      <td>83.931498</td>\n",
       "      <td>0.018783</td>\n",
       "      <td>0.012403</td>\n",
       "      <td>80.260124</td>\n",
       "      <td>0.002067</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>80.260124</td>\n",
       "      <td>80.425666</td>\n",
       "      <td>78.694953</td>\n",
       "      <td>80.184871</td>\n",
       "      <td>493585800</td>\n",
       "      <td>0.002067</td>\n",
       "      <td>0.002064</td>\n",
       "      <td>81.625131</td>\n",
       "      <td>83.648186</td>\n",
       "      <td>0.018457</td>\n",
       "      <td>0.012344</td>\n",
       "      <td>79.680710</td>\n",
       "      <td>-0.007219</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-02-08</td>\n",
       "      <td>79.680710</td>\n",
       "      <td>80.764291</td>\n",
       "      <td>79.620510</td>\n",
       "      <td>80.320322</td>\n",
       "      <td>224166900</td>\n",
       "      <td>-0.007219</td>\n",
       "      <td>-0.007245</td>\n",
       "      <td>81.148059</td>\n",
       "      <td>83.321606</td>\n",
       "      <td>0.015917</td>\n",
       "      <td>0.012270</td>\n",
       "      <td>80.681526</td>\n",
       "      <td>0.012560</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date      Close       High        Low       Open     Volume    ret_1d  \\\n",
       "0 2010-02-02  83.059364  83.217386  81.930636  82.216584  216327900  0.012104   \n",
       "1 2010-02-03  82.645493  83.134609  82.404697  82.683113  172730700 -0.004983   \n",
       "2 2010-02-04  80.094604  82.043540  80.079552  82.005919  356715700 -0.030865   \n",
       "3 2010-02-05  80.260124  80.425666  78.694953  80.184871  493585800  0.002067   \n",
       "4 2010-02-08  79.680710  80.764291  79.620510  80.320322  224166900 -0.007219   \n",
       "\n",
       "   log_ret_1d  ma_close_5  ma_close_20     vol_5    vol_20  future_price  \\\n",
       "0    0.012031   82.055548    84.347997  0.012653  0.010585     82.645493   \n",
       "1   -0.004995   82.055548    84.205024  0.012873  0.010574     80.094604   \n",
       "2   -0.031352   81.734995    83.931498  0.018783  0.012403     80.260124   \n",
       "3    0.002064   81.625131    83.648186  0.018457  0.012344     79.680710   \n",
       "4   -0.007245   81.148059    83.321606  0.015917  0.012270     80.681526   \n",
       "\n",
       "   future_ret_1d  label_up  \n",
       "0      -0.004983         0  \n",
       "1      -0.030865         0  \n",
       "2       0.002067         1  \n",
       "3      -0.007219         0  \n",
       "4       0.012560         1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (3753, 15)\n",
      "Date range: 2010-02-02 00:00:00 → 2024-12-30 00:00:00\n",
      "\n",
      "Label distribution:\n",
      "label_up\n",
      "1    0.552625\n",
      "0    0.447375\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "daily_path = DATA_PROC / \"daily_merged.parquet\"\n",
    "df = pd.read_parquet(daily_path)\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "print(\"=== DAILY SPY (BASE) ===\")\n",
    "display(df.head())\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Date range:\", df[\"date\"].min(), \"→\", df[\"date\"].max())\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(df[\"label_up\"].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed1b2d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  3 of 3 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: ['^VIX', '^TNX', '^IRX']\n",
      "=== RAW MACRO DATA ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Close</th>\n",
       "      <th colspan=\"3\" halign=\"left\">High</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Low</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Open</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>^IRX</th>\n",
       "      <th>^TNX</th>\n",
       "      <th>^VIX</th>\n",
       "      <th>^IRX</th>\n",
       "      <th>^TNX</th>\n",
       "      <th>^VIX</th>\n",
       "      <th>^IRX</th>\n",
       "      <th>^TNX</th>\n",
       "      <th>^VIX</th>\n",
       "      <th>^IRX</th>\n",
       "      <th>^TNX</th>\n",
       "      <th>^VIX</th>\n",
       "      <th>^IRX</th>\n",
       "      <th>^TNX</th>\n",
       "      <th>^VIX</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-02-02</th>\n",
       "      <td>0.090</td>\n",
       "      <td>3.635</td>\n",
       "      <td>21.480000</td>\n",
       "      <td>0.095</td>\n",
       "      <td>3.656</td>\n",
       "      <td>22.990000</td>\n",
       "      <td>0.090</td>\n",
       "      <td>3.633</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>0.090</td>\n",
       "      <td>3.650</td>\n",
       "      <td>22.590000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-03</th>\n",
       "      <td>0.090</td>\n",
       "      <td>3.703</td>\n",
       "      <td>21.600000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>3.709</td>\n",
       "      <td>22.110001</td>\n",
       "      <td>0.090</td>\n",
       "      <td>3.666</td>\n",
       "      <td>21.330000</td>\n",
       "      <td>0.090</td>\n",
       "      <td>3.679</td>\n",
       "      <td>22.110001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-04</th>\n",
       "      <td>0.085</td>\n",
       "      <td>3.610</td>\n",
       "      <td>26.080000</td>\n",
       "      <td>0.095</td>\n",
       "      <td>3.683</td>\n",
       "      <td>26.320000</td>\n",
       "      <td>0.085</td>\n",
       "      <td>3.594</td>\n",
       "      <td>22.629999</td>\n",
       "      <td>0.090</td>\n",
       "      <td>3.683</td>\n",
       "      <td>22.629999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-05</th>\n",
       "      <td>0.085</td>\n",
       "      <td>3.546</td>\n",
       "      <td>26.110001</td>\n",
       "      <td>0.085</td>\n",
       "      <td>3.645</td>\n",
       "      <td>29.219999</td>\n",
       "      <td>0.075</td>\n",
       "      <td>3.537</td>\n",
       "      <td>25.370001</td>\n",
       "      <td>0.075</td>\n",
       "      <td>3.585</td>\n",
       "      <td>25.690001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-08</th>\n",
       "      <td>0.090</td>\n",
       "      <td>3.592</td>\n",
       "      <td>26.510000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>3.602</td>\n",
       "      <td>27.110001</td>\n",
       "      <td>0.080</td>\n",
       "      <td>3.571</td>\n",
       "      <td>25.480000</td>\n",
       "      <td>0.090</td>\n",
       "      <td>3.577</td>\n",
       "      <td>26.110001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Price       Close                     High                      Low         \\\n",
       "Ticker       ^IRX   ^TNX       ^VIX   ^IRX   ^TNX       ^VIX   ^IRX   ^TNX   \n",
       "Date                                                                         \n",
       "2010-02-02  0.090  3.635  21.480000  0.095  3.656  22.990000  0.090  3.633   \n",
       "2010-02-03  0.090  3.703  21.600000  0.100  3.709  22.110001  0.090  3.666   \n",
       "2010-02-04  0.085  3.610  26.080000  0.095  3.683  26.320000  0.085  3.594   \n",
       "2010-02-05  0.085  3.546  26.110001  0.085  3.645  29.219999  0.075  3.537   \n",
       "2010-02-08  0.090  3.592  26.510000  0.100  3.602  27.110001  0.080  3.571   \n",
       "\n",
       "Price                   Open                   Volume            \n",
       "Ticker           ^VIX   ^IRX   ^TNX       ^VIX   ^IRX ^TNX ^VIX  \n",
       "Date                                                             \n",
       "2010-02-02  21.080000  0.090  3.650  22.590000    0.0  0.0    0  \n",
       "2010-02-03  21.330000  0.090  3.679  22.110001    0.0  0.0    0  \n",
       "2010-02-04  22.629999  0.090  3.683  22.629999    0.0  0.0    0  \n",
       "2010-02-05  25.370001  0.075  3.585  25.690001    0.0  0.0    0  \n",
       "2010-02-08  25.480000  0.090  3.577  26.110001    0.0  0.0    0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (3752, 15)\n"
     ]
    }
   ],
   "source": [
    "start_date = df[\"date\"].min()\n",
    "end_date = df[\"date\"].max()\n",
    "\n",
    "macro_tickers = [\"^VIX\", \"^TNX\", \"^IRX\"]\n",
    "print(\"Downloading:\", macro_tickers)\n",
    "\n",
    "data_macro = yf.download(\n",
    "    tickers=\" \".join(macro_tickers),\n",
    "    start=start_date,\n",
    "    end=end_date,\n",
    "    interval=\"1d\",\n",
    "    auto_adjust=True,\n",
    ")\n",
    "\n",
    "# data_macro has columns like ('Adj Close', '^VIX'), ('Adj Close', '^TNX'), etc.\n",
    "print(\"=== RAW MACRO DATA ===\")\n",
    "display(data_macro.head())\n",
    "print(\"Shape:\", data_macro.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8bdd7af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MACRO ADJ CLOSE ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>irx_3m</th>\n",
       "      <th>tnx_10y</th>\n",
       "      <th>vix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-02-02</td>\n",
       "      <td>0.090</td>\n",
       "      <td>3.635</td>\n",
       "      <td>21.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-02-03</td>\n",
       "      <td>0.090</td>\n",
       "      <td>3.703</td>\n",
       "      <td>21.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-02-04</td>\n",
       "      <td>0.085</td>\n",
       "      <td>3.610</td>\n",
       "      <td>26.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>0.085</td>\n",
       "      <td>3.546</td>\n",
       "      <td>26.110001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-02-08</td>\n",
       "      <td>0.090</td>\n",
       "      <td>3.592</td>\n",
       "      <td>26.510000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Ticker       date  irx_3m  tnx_10y        vix\n",
       "0      2010-02-02   0.090    3.635  21.480000\n",
       "1      2010-02-03   0.090    3.703  21.600000\n",
       "2      2010-02-04   0.085    3.610  26.080000\n",
       "3      2010-02-05   0.085    3.546  26.110001\n",
       "4      2010-02-08   0.090    3.592  26.510000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MACRO FEATURES PREVIEW ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>irx_3m</th>\n",
       "      <th>tnx_10y</th>\n",
       "      <th>vix</th>\n",
       "      <th>term_spread</th>\n",
       "      <th>log_vix</th>\n",
       "      <th>vix_change_1d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-02-02</td>\n",
       "      <td>0.090</td>\n",
       "      <td>3.635</td>\n",
       "      <td>21.480000</td>\n",
       "      <td>3.545</td>\n",
       "      <td>3.067122</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-02-03</td>\n",
       "      <td>0.090</td>\n",
       "      <td>3.703</td>\n",
       "      <td>21.600000</td>\n",
       "      <td>3.613</td>\n",
       "      <td>3.072693</td>\n",
       "      <td>0.005587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-02-04</td>\n",
       "      <td>0.085</td>\n",
       "      <td>3.610</td>\n",
       "      <td>26.080000</td>\n",
       "      <td>3.525</td>\n",
       "      <td>3.261169</td>\n",
       "      <td>0.207407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>0.085</td>\n",
       "      <td>3.546</td>\n",
       "      <td>26.110001</td>\n",
       "      <td>3.461</td>\n",
       "      <td>3.262318</td>\n",
       "      <td>0.001150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-02-08</td>\n",
       "      <td>0.090</td>\n",
       "      <td>3.592</td>\n",
       "      <td>26.510000</td>\n",
       "      <td>3.502</td>\n",
       "      <td>3.277522</td>\n",
       "      <td>0.015320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Ticker       date  irx_3m  tnx_10y        vix  term_spread   log_vix  \\\n",
       "0      2010-02-02   0.090    3.635  21.480000        3.545  3.067122   \n",
       "1      2010-02-03   0.090    3.703  21.600000        3.613  3.072693   \n",
       "2      2010-02-04   0.085    3.610  26.080000        3.525  3.261169   \n",
       "3      2010-02-05   0.085    3.546  26.110001        3.461  3.262318   \n",
       "4      2010-02-08   0.090    3.592  26.510000        3.502  3.277522   \n",
       "\n",
       "Ticker  vix_change_1d  \n",
       "0                 NaN  \n",
       "1            0.005587  \n",
       "2            0.207407  \n",
       "3            0.001150  \n",
       "4            0.015320  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (3752, 7)\n"
     ]
    }
   ],
   "source": [
    "# Keep only Adj Close level\n",
    "adj_close = data_macro[\"Close\"].copy()\n",
    "adj_close = adj_close.rename(columns={\n",
    "    \"^VIX\": \"vix\",\n",
    "    \"^TNX\": \"tnx_10y\",\n",
    "    \"^IRX\": \"irx_3m\",\n",
    "})\n",
    "\n",
    "# Index is DatetimeIndex; reset to column 'date'\n",
    "adj_close = adj_close.reset_index().rename(columns={\"Date\": \"date\"})\n",
    "adj_close[\"date\"] = pd.to_datetime(adj_close[\"date\"])\n",
    "\n",
    "print(\"=== MACRO ADJ CLOSE ===\")\n",
    "display(adj_close.head())\n",
    "\n",
    "# Build macro features\n",
    "macro = adj_close.copy()\n",
    "macro[\"term_spread\"] = macro[\"tnx_10y\"] - macro[\"irx_3m\"]\n",
    "macro[\"log_vix\"] = np.log(macro[\"vix\"].replace(0, np.nan))\n",
    "macro[\"vix_change_1d\"] = macro[\"vix\"].pct_change()\n",
    "\n",
    "print(\"=== MACRO FEATURES PREVIEW ===\")\n",
    "display(macro.head())\n",
    "print(\"Shape:\", macro.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c310aa4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DAILY + MACRO MERGED ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "      <th>ret_1d</th>\n",
       "      <th>log_ret_1d</th>\n",
       "      <th>ma_close_5</th>\n",
       "      <th>ma_close_20</th>\n",
       "      <th>...</th>\n",
       "      <th>vol_20</th>\n",
       "      <th>future_price</th>\n",
       "      <th>future_ret_1d</th>\n",
       "      <th>label_up</th>\n",
       "      <th>irx_3m</th>\n",
       "      <th>tnx_10y</th>\n",
       "      <th>vix</th>\n",
       "      <th>term_spread</th>\n",
       "      <th>log_vix</th>\n",
       "      <th>vix_change_1d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-02-02</td>\n",
       "      <td>83.059364</td>\n",
       "      <td>83.217386</td>\n",
       "      <td>81.930636</td>\n",
       "      <td>82.216584</td>\n",
       "      <td>216327900</td>\n",
       "      <td>0.012104</td>\n",
       "      <td>0.012031</td>\n",
       "      <td>82.055548</td>\n",
       "      <td>84.347997</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010585</td>\n",
       "      <td>82.645493</td>\n",
       "      <td>-0.004983</td>\n",
       "      <td>0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>3.635</td>\n",
       "      <td>21.480000</td>\n",
       "      <td>3.545</td>\n",
       "      <td>3.067122</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-02-03</td>\n",
       "      <td>82.645493</td>\n",
       "      <td>83.134609</td>\n",
       "      <td>82.404697</td>\n",
       "      <td>82.683113</td>\n",
       "      <td>172730700</td>\n",
       "      <td>-0.004983</td>\n",
       "      <td>-0.004995</td>\n",
       "      <td>82.055548</td>\n",
       "      <td>84.205024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010574</td>\n",
       "      <td>80.094604</td>\n",
       "      <td>-0.030865</td>\n",
       "      <td>0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>3.703</td>\n",
       "      <td>21.600000</td>\n",
       "      <td>3.613</td>\n",
       "      <td>3.072693</td>\n",
       "      <td>0.005587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-02-04</td>\n",
       "      <td>80.094604</td>\n",
       "      <td>82.043540</td>\n",
       "      <td>80.079552</td>\n",
       "      <td>82.005919</td>\n",
       "      <td>356715700</td>\n",
       "      <td>-0.030865</td>\n",
       "      <td>-0.031352</td>\n",
       "      <td>81.734995</td>\n",
       "      <td>83.931498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012403</td>\n",
       "      <td>80.260124</td>\n",
       "      <td>0.002067</td>\n",
       "      <td>1</td>\n",
       "      <td>0.085</td>\n",
       "      <td>3.610</td>\n",
       "      <td>26.080000</td>\n",
       "      <td>3.525</td>\n",
       "      <td>3.261169</td>\n",
       "      <td>0.207407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>80.260124</td>\n",
       "      <td>80.425666</td>\n",
       "      <td>78.694953</td>\n",
       "      <td>80.184871</td>\n",
       "      <td>493585800</td>\n",
       "      <td>0.002067</td>\n",
       "      <td>0.002064</td>\n",
       "      <td>81.625131</td>\n",
       "      <td>83.648186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012344</td>\n",
       "      <td>79.680710</td>\n",
       "      <td>-0.007219</td>\n",
       "      <td>0</td>\n",
       "      <td>0.085</td>\n",
       "      <td>3.546</td>\n",
       "      <td>26.110001</td>\n",
       "      <td>3.461</td>\n",
       "      <td>3.262318</td>\n",
       "      <td>0.001150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-02-08</td>\n",
       "      <td>79.680710</td>\n",
       "      <td>80.764291</td>\n",
       "      <td>79.620510</td>\n",
       "      <td>80.320322</td>\n",
       "      <td>224166900</td>\n",
       "      <td>-0.007219</td>\n",
       "      <td>-0.007245</td>\n",
       "      <td>81.148059</td>\n",
       "      <td>83.321606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012270</td>\n",
       "      <td>80.681526</td>\n",
       "      <td>0.012560</td>\n",
       "      <td>1</td>\n",
       "      <td>0.090</td>\n",
       "      <td>3.592</td>\n",
       "      <td>26.510000</td>\n",
       "      <td>3.502</td>\n",
       "      <td>3.277522</td>\n",
       "      <td>0.015320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date      Close       High        Low       Open     Volume    ret_1d  \\\n",
       "0 2010-02-02  83.059364  83.217386  81.930636  82.216584  216327900  0.012104   \n",
       "1 2010-02-03  82.645493  83.134609  82.404697  82.683113  172730700 -0.004983   \n",
       "2 2010-02-04  80.094604  82.043540  80.079552  82.005919  356715700 -0.030865   \n",
       "3 2010-02-05  80.260124  80.425666  78.694953  80.184871  493585800  0.002067   \n",
       "4 2010-02-08  79.680710  80.764291  79.620510  80.320322  224166900 -0.007219   \n",
       "\n",
       "   log_ret_1d  ma_close_5  ma_close_20  ...    vol_20  future_price  \\\n",
       "0    0.012031   82.055548    84.347997  ...  0.010585     82.645493   \n",
       "1   -0.004995   82.055548    84.205024  ...  0.010574     80.094604   \n",
       "2   -0.031352   81.734995    83.931498  ...  0.012403     80.260124   \n",
       "3    0.002064   81.625131    83.648186  ...  0.012344     79.680710   \n",
       "4   -0.007245   81.148059    83.321606  ...  0.012270     80.681526   \n",
       "\n",
       "   future_ret_1d  label_up  irx_3m  tnx_10y        vix  term_spread   log_vix  \\\n",
       "0      -0.004983         0   0.090    3.635  21.480000        3.545  3.067122   \n",
       "1      -0.030865         0   0.090    3.703  21.600000        3.613  3.072693   \n",
       "2       0.002067         1   0.085    3.610  26.080000        3.525  3.261169   \n",
       "3      -0.007219         0   0.085    3.546  26.110001        3.461  3.262318   \n",
       "4       0.012560         1   0.090    3.592  26.510000        3.502  3.277522   \n",
       "\n",
       "   vix_change_1d  \n",
       "0            NaN  \n",
       "1       0.005587  \n",
       "2       0.207407  \n",
       "3       0.001150  \n",
       "4       0.015320  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (3753, 21)\n",
      "Date range: 2010-02-02 00:00:00 → 2024-12-30 00:00:00\n",
      "\n",
      "Label distribution:\n",
      "label_up\n",
      "1    0.552625\n",
      "0    0.447375\n",
      "Name: proportion, dtype: float64\n",
      "Saved: C:\\Users\\KDP only\\Documents\\ANN_Final_Project\\spy-ann\\data\\processed\\daily_with_macro.parquet\n"
     ]
    }
   ],
   "source": [
    "df_merged = df.merge(macro, on=\"date\", how=\"left\")\n",
    "\n",
    "# forward-fill macro columns\n",
    "macro_cols = [\"vix\", \"tnx_10y\", \"irx_3m\", \"term_spread\", \"log_vix\", \"vix_change_1d\"]\n",
    "df_merged[macro_cols] = df_merged[macro_cols].ffill()\n",
    "\n",
    "print(\"=== DAILY + MACRO MERGED ===\")\n",
    "display(df_merged.head())\n",
    "print(\"Shape:\", df_merged.shape)\n",
    "print(\"Date range:\", df_merged[\"date\"].min(), \"→\", df_merged[\"date\"].max())\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(df_merged[\"label_up\"].value_counts(normalize=True))\n",
    "\n",
    "out_path = DATA_PROC / \"daily_with_macro.parquet\"\n",
    "df_merged.to_parquet(out_path)\n",
    "print(\"Saved:\", out_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06d8ad85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns:\n",
      "['Close', 'High', 'Low', 'Open', 'Volume', 'ret_1d', 'log_ret_1d', 'ma_close_5', 'ma_close_20', 'vol_5', 'vol_20', 'irx_3m', 'tnx_10y', 'vix', 'term_spread', 'log_vix', 'vix_change_1d']\n",
      "Number of features: 17\n",
      "Raw split sizes:\n",
      "Train: 2244\n",
      "Val  : 757\n",
      "Test : 752\n",
      "\n",
      "Label distribution (train/val/test):\n",
      "Train\n",
      "label_up\n",
      "1    0.550802\n",
      "0    0.449198\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Val\n",
      "label_up\n",
      "1    0.583884\n",
      "0    0.416116\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test\n",
      "label_up\n",
      "1    0.526596\n",
      "0    0.473404\n",
      "Name: proportion, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfm = df_merged.copy()\n",
    "\n",
    "drop_cols = [\"date\", \"future_price\", \"future_ret_1d\", \"label_up\"]\n",
    "feature_cols = [c for c in dfm.columns if c not in drop_cols]\n",
    "\n",
    "print(\"Feature columns:\")\n",
    "print(feature_cols)\n",
    "print(\"Number of features:\", len(feature_cols))\n",
    "\n",
    "X_all = dfm[feature_cols].values.astype(\"float32\")\n",
    "y_all = dfm[\"label_up\"].values.astype(\"float32\")\n",
    "\n",
    "train_end_date = pd.Timestamp(\"2018-12-31\")\n",
    "val_end_date   = pd.Timestamp(\"2021-12-31\")\n",
    "\n",
    "train_mask = dfm[\"date\"] <= train_end_date\n",
    "val_mask   = (dfm[\"date\"] > train_end_date) & (dfm[\"date\"] <= val_end_date)\n",
    "test_mask  = dfm[\"date\"] > val_end_date\n",
    "\n",
    "X_train = X_all[train_mask.values]\n",
    "y_train = y_all[train_mask.values]\n",
    "\n",
    "X_val = X_all[val_mask.values]\n",
    "y_val = y_all[val_mask.values]\n",
    "\n",
    "X_test = X_all[test_mask.values]\n",
    "y_test = y_all[test_mask.values]\n",
    "\n",
    "print(\"Raw split sizes:\")\n",
    "print(\"Train:\", X_train.shape[0])\n",
    "print(\"Val  :\", X_val.shape[0])\n",
    "print(\"Test :\", X_test.shape[0])\n",
    "\n",
    "print(\"\\nLabel distribution (train/val/test):\")\n",
    "for name, mask in [(\"Train\", train_mask), (\"Val\", val_mask), (\"Test\", test_mask)]:\n",
    "    print(name)\n",
    "    print(dfm.loc[mask, \"label_up\"].value_counts(normalize=True))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7920076b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled   = scaler.transform(X_val)\n",
    "X_test_scaled  = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4674eb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_windows(X, y, window_size: int):\n",
    "    \"\"\"\n",
    "    X: (N, F), y: (N,)\n",
    "    Returns:\n",
    "        X_seq: (N_windows, W, F)\n",
    "        y_seq: (N_windows,)\n",
    "    Window uses rows [i, ..., i+W-1], target is y[i+W]\n",
    "    \"\"\"\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "\n",
    "    n = len(X)\n",
    "    max_start = n - window_size - 1\n",
    "\n",
    "    if max_start < 0:\n",
    "        return np.empty((0, window_size, X.shape[1]), dtype=X.dtype), np.empty((0,), dtype=y.dtype)\n",
    "\n",
    "    seqs, targets = [], []\n",
    "    for i in range(max_start + 1):\n",
    "        seq = X[i : i + window_size]\n",
    "        target = y[i + window_size]\n",
    "        seqs.append(seq)\n",
    "        targets.append(target)\n",
    "\n",
    "    X_seq = np.stack(seqs, axis=0)\n",
    "    y_seq = np.array(targets, dtype=y.dtype)\n",
    "    return X_seq, y_seq\n",
    "\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, X_seq: np.ndarray, y_seq: np.ndarray):\n",
    "        self.X = X_seq\n",
    "        self.y = y_seq\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.X[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.y[idx], dtype=torch.float32)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c2773de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPWindow(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_sizes=(64, 32)):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = input_dim\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, W, F) → flatten\n",
    "        b, w, f = x.shape\n",
    "        x = x.view(b, w * f)\n",
    "        logits = self.net(x).squeeze(-1)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def train_mlp_macro(\n",
    "    window_size: int,\n",
    "    hidden_sizes=(64, 32),\n",
    "    batch_size: int = 64,\n",
    "    num_epochs: int = 25,\n",
    "    lr: float = 1e-3,\n",
    "    use_scaled: bool = True,\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train MLP on price+macro features, with option to use scaled or raw inputs.\n",
    "\n",
    "    Assumes you have:\n",
    "        X_train, X_val, X_test (raw)\n",
    "        X_train_scaled, X_val_scaled, X_test_scaled (StandardScaler)\n",
    "        y_train, y_val, y_test\n",
    "    defined in the notebook scope.\n",
    "    \"\"\"\n",
    "\n",
    "    if use_scaled:\n",
    "        Xtr_base, Xval_base, Xte_base = X_train_scaled, X_val_scaled, X_test_scaled\n",
    "        scale_mode = \"scaled\"\n",
    "    else:\n",
    "        Xtr_base, Xval_base, Xte_base = X_train, X_val, X_test\n",
    "        scale_mode = \"raw\"\n",
    "\n",
    "    Xtr_seq, ytr_seq = build_windows(Xtr_base, y_train, window_size)\n",
    "    Xval_seq, yval_seq = build_windows(Xval_base, y_val, window_size)\n",
    "    Xte_seq, yte_seq = build_windows(Xte_base, y_test, window_size)\n",
    "\n",
    "    print(\n",
    "        f\"\\n=== MLP price+macro ({scale_mode}) \"\n",
    "        f\"W={window_size}, hidden={hidden_sizes} ===\"\n",
    "    )\n",
    "    print(\"Train windows:\", Xtr_seq.shape[0], \"Val:\", Xval_seq.shape[0], \"Test:\", Xte_seq.shape[0])\n",
    "\n",
    "    if Xtr_seq.shape[0] == 0 or Xval_seq.shape[0] == 0 or Xte_seq.shape[0] == 0:\n",
    "        print(\"Not enough data for this window size; skipping.\")\n",
    "        return {\n",
    "            \"model\": \"MLP\",\n",
    "            \"features\": \"price+macro\",\n",
    "            \"window_size\": window_size,\n",
    "            \"hidden_desc\": str(hidden_sizes),\n",
    "            \"scaled\": use_scaled,\n",
    "            \"best_val_acc\": float(\"nan\"),\n",
    "            \"test_acc\": float(\"nan\"),\n",
    "        }\n",
    "\n",
    "    train_ds = SeqDataset(Xtr_seq, ytr_seq)\n",
    "    val_ds   = SeqDataset(Xval_seq, yval_seq)\n",
    "    test_ds  = SeqDataset(Xte_seq, yte_seq)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    input_dim = Xtr_seq.shape[1] * Xtr_seq.shape[2]\n",
    "    model = MLPWindow(input_dim=input_dim, hidden_sizes=hidden_sizes).to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # ---- Train ----\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_examples = 0\n",
    "\n",
    "        for Xb, yb in train_loader:\n",
    "            Xb = Xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(Xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * yb.size(0)\n",
    "            preds = (torch.sigmoid(logits) >= 0.5).float()\n",
    "            total_correct += (preds == yb).float().sum().item()\n",
    "            total_examples += yb.size(0)\n",
    "\n",
    "        train_loss = total_loss / total_examples\n",
    "        train_acc = total_correct / total_examples\n",
    "\n",
    "        # ---- Val ----\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_examples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for Xb, yb in val_loader:\n",
    "                Xb = Xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "\n",
    "                logits = model(Xb)\n",
    "                loss = criterion(logits, yb)\n",
    "\n",
    "                val_loss += loss.item() * yb.size(0)\n",
    "                preds = (torch.sigmoid(logits) >= 0.5).float()\n",
    "                val_correct += (preds == yb).float().sum().item()\n",
    "                val_examples += yb.size(0)\n",
    "\n",
    "        val_loss /= val_examples\n",
    "        val_acc = val_correct / val_examples\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_state = model.state_dict()\n",
    "\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"Epoch {epoch:02d} | \"\n",
    "                f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
    "                f\"val_loss={val_loss:.4f} acc={val_acc:.4f}\"\n",
    "            )\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    # ---- Test ----\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_examples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for Xb, yb in test_loader:\n",
    "            Xb = Xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            logits = model(Xb)\n",
    "            preds = (torch.sigmoid(logits) >= 0.5).float()\n",
    "            test_correct += (preds == yb).float().sum().item()\n",
    "            test_examples += yb.size(0)\n",
    "\n",
    "    test_acc = test_correct / test_examples if test_examples > 0 else float(\"nan\")\n",
    "    print(f\"[{scale_mode}] Best val acc={best_val_acc:.4f} | Test acc={test_acc:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"model\": \"MLP\",\n",
    "        \"features\": \"price+macro\",\n",
    "        \"window_size\": window_size,\n",
    "        \"hidden_desc\": str(hidden_sizes),\n",
    "        \"scaled\": use_scaled,\n",
    "        \"best_val_acc\": best_val_acc,\n",
    "        \"test_acc\": test_acc,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6695ecfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int = 32, num_layers: int = 1, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, W, F)\n",
    "        out, h_n = self.gru(x)\n",
    "        last_hidden = h_n[-1]\n",
    "        logits = self.fc(last_hidden).squeeze(-1)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def train_gru_macro(\n",
    "    window_size: int,\n",
    "    hidden_dim: int,\n",
    "    num_layers: int = 1,\n",
    "    batch_size: int = 64,\n",
    "    num_epochs: int = 25,\n",
    "    lr: float = 1e-3,\n",
    "    use_scaled: bool = True,\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train GRU on price+macro features, with option to use scaled or raw inputs.\n",
    "\n",
    "    Assumes you have:\n",
    "        X_train, X_val, X_test (raw)\n",
    "        X_train_scaled, X_val_scaled, X_test_scaled (StandardScaler)\n",
    "        y_train, y_val, y_test\n",
    "    defined in the notebook scope.\n",
    "    \"\"\"\n",
    "\n",
    "    if use_scaled:\n",
    "        Xtr_base, Xval_base, Xte_base = X_train_scaled, X_val_scaled, X_test_scaled\n",
    "        scale_mode = \"scaled\"\n",
    "    else:\n",
    "        Xtr_base, Xval_base, Xte_base = X_train, X_val, X_test\n",
    "        scale_mode = \"raw\"\n",
    "\n",
    "    Xtr_seq, ytr_seq = build_windows(Xtr_base, y_train, window_size)\n",
    "    Xval_seq, yval_seq = build_windows(Xval_base, y_val, window_size)\n",
    "    Xte_seq, yte_seq = build_windows(Xte_base, y_test, window_size)\n",
    "\n",
    "    print(\n",
    "        f\"\\n=== GRU price+macro ({scale_mode}) \"\n",
    "        f\"W={window_size}, hidden={hidden_dim}, layers={num_layers} ===\"\n",
    "    )\n",
    "    print(\"Train windows:\", Xtr_seq.shape[0], \"Val:\", Xval_seq.shape[0], \"Test:\", Xte_seq.shape[0])\n",
    "\n",
    "    if Xtr_seq.shape[0] == 0 or Xval_seq.shape[0] == 0 or Xte_seq.shape[0] == 0:\n",
    "        print(\"Not enough data for this window size; skipping.\")\n",
    "        return {\n",
    "            \"model\": \"GRU\",\n",
    "            \"features\": \"price+macro\",\n",
    "            \"window_size\": window_size,\n",
    "            \"hidden_desc\": f\"hidden={hidden_dim}, layers={num_layers}\",\n",
    "            \"scaled\": use_scaled,\n",
    "            \"best_val_acc\": float(\"nan\"),\n",
    "            \"test_acc\": float(\"nan\"),\n",
    "        }\n",
    "\n",
    "    train_ds = SeqDataset(Xtr_seq, ytr_seq)\n",
    "    val_ds   = SeqDataset(Xval_seq, yval_seq)\n",
    "    test_ds  = SeqDataset(Xte_seq, yte_seq)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    input_dim = Xtr_seq.shape[2]\n",
    "    model = GRUNet(input_dim=input_dim, hidden_dim=hidden_dim, num_layers=num_layers).to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # ---- Train ----\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_examples = 0\n",
    "\n",
    "        for Xb, yb in train_loader:\n",
    "            Xb = Xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(Xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * yb.size(0)\n",
    "            preds = (torch.sigmoid(logits) >= 0.5).float()\n",
    "            total_correct += (preds == yb).float().sum().item()\n",
    "            total_examples += yb.size(0)\n",
    "\n",
    "        train_loss = total_loss / total_examples\n",
    "        train_acc = total_correct / total_examples\n",
    "\n",
    "        # ---- Val ----\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_examples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for Xb, yb in val_loader:\n",
    "                Xb = Xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "\n",
    "                logits = model(Xb)\n",
    "                loss = criterion(logits, yb)\n",
    "\n",
    "                val_loss += loss.item() * yb.size(0)\n",
    "                preds = (torch.sigmoid(logits) >= 0.5).float()\n",
    "                val_correct += (preds == yb).float().sum().item()\n",
    "                val_examples += yb.size(0)\n",
    "\n",
    "        val_loss /= val_examples\n",
    "        val_acc = val_correct / val_examples\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_state = model.state_dict()\n",
    "\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"Epoch {epoch:02d} | \"\n",
    "                f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
    "                f\"val_loss={val_loss:.4f} acc={val_acc:.4f}\"\n",
    "            )\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    # ---- Test ----\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_examples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for Xb, yb in test_loader:\n",
    "            Xb = Xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            logits = model(Xb)\n",
    "            preds = (torch.sigmoid(logits) >= 0.5).float()\n",
    "            test_correct += (preds == yb).float().sum().item()\n",
    "            test_examples += yb.size(0)\n",
    "\n",
    "    test_acc = test_correct / test_examples if test_examples > 0 else float(\"nan\")\n",
    "    print(f\"[{scale_mode}] Best val acc={best_val_acc:.4f} | Test acc={test_acc:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"model\": \"GRU\",\n",
    "        \"features\": \"price+macro\",\n",
    "        \"window_size\": window_size,\n",
    "        \"hidden_desc\": f\"hidden={hidden_dim}, layers={num_layers}\",\n",
    "        \"scaled\": use_scaled,\n",
    "        \"best_val_acc\": best_val_acc,\n",
    "        \"test_acc\": test_acc,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9c940e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int = 32, num_layers: int = 1, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, W, F)\n",
    "        out, (h_n, c_n) = self.lstm(x)\n",
    "        last_hidden = h_n[-1]  # (batch, hidden_dim)\n",
    "        logits = self.fc(last_hidden).squeeze(-1)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def train_lstm_macro(\n",
    "    window_size: int,\n",
    "    hidden_dim: int,\n",
    "    num_layers: int = 1,\n",
    "    batch_size: int = 64,\n",
    "    num_epochs: int = 25,\n",
    "    lr: float = 1e-3,\n",
    "    use_scaled: bool = True,\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train LSTM on price+macro features, with option to use scaled or raw inputs.\n",
    "\n",
    "    Assumes:\n",
    "        X_train, X_val, X_test (raw)\n",
    "        X_train_scaled, X_val_scaled, X_test_scaled (scaled)\n",
    "        y_train, y_val, y_test\n",
    "    exist in the notebook.\n",
    "    \"\"\"\n",
    "\n",
    "    if use_scaled:\n",
    "        Xtr_base, Xval_base, Xte_base = X_train_scaled, X_val_scaled, X_test_scaled\n",
    "        scale_mode = \"scaled\"\n",
    "    else:\n",
    "        Xtr_base, Xval_base, Xte_base = X_train, X_val, X_test\n",
    "        scale_mode = \"raw\"\n",
    "\n",
    "    Xtr_seq, ytr_seq = build_windows(Xtr_base, y_train, window_size)\n",
    "    Xval_seq, yval_seq = build_windows(Xval_base, y_val, window_size)\n",
    "    Xte_seq, yte_seq = build_windows(Xte_base, y_test, window_size)\n",
    "\n",
    "    print(\n",
    "        f\"\\n=== LSTM price+macro ({scale_mode}) \"\n",
    "        f\"W={window_size}, hidden={hidden_dim}, layers={num_layers} ===\"\n",
    "    )\n",
    "    print(\"Train windows:\", Xtr_seq.shape[0], \"Val:\", Xval_seq.shape[0], \"Test:\", Xte_seq.shape[0])\n",
    "\n",
    "    if Xtr_seq.shape[0] == 0 or Xval_seq.shape[0] == 0 or Xte_seq.shape[0] == 0:\n",
    "        print(\"Not enough data for this window size; skipping.\")\n",
    "        return {\n",
    "            \"model\": \"LSTM\",\n",
    "            \"features\": \"price+macro\",\n",
    "            \"window_size\": window_size,\n",
    "            \"hidden_desc\": f\"hidden={hidden_dim}, layers={num_layers}\",\n",
    "            \"scaled\": use_scaled,\n",
    "            \"best_val_acc\": float(\"nan\"),\n",
    "            \"test_acc\": float(\"nan\"),\n",
    "        }\n",
    "\n",
    "    train_ds = SeqDataset(Xtr_seq, ytr_seq)\n",
    "    val_ds   = SeqDataset(Xval_seq, yval_seq)\n",
    "    test_ds  = SeqDataset(Xte_seq, yte_seq)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    input_dim = Xtr_seq.shape[2]\n",
    "    model = LSTMNet(input_dim=input_dim, hidden_dim=hidden_dim, num_layers=num_layers).to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # ---- Train ----\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_examples = 0\n",
    "\n",
    "        for Xb, yb in train_loader:\n",
    "            Xb = Xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(Xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * yb.size(0)\n",
    "            preds = (torch.sigmoid(logits) >= 0.5).float()\n",
    "            total_correct += (preds == yb).float().sum().item()\n",
    "            total_examples += yb.size(0)\n",
    "\n",
    "        train_loss = total_loss / total_examples\n",
    "        train_acc = total_correct / total_examples\n",
    "\n",
    "        # ---- Val ----\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_examples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for Xb, yb in val_loader:\n",
    "                Xb = Xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "\n",
    "                logits = model(Xb)\n",
    "                loss = criterion(logits, yb)\n",
    "\n",
    "                val_loss += loss.item() * yb.size(0)\n",
    "                preds = (torch.sigmoid(logits) >= 0.5).float()\n",
    "                val_correct += (preds == yb).float().sum().item()\n",
    "                val_examples += yb.size(0)\n",
    "\n",
    "        val_loss /= val_examples\n",
    "        val_acc = val_correct / val_examples\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_state = model.state_dict()\n",
    "\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"Epoch {epoch:02d} | \"\n",
    "                f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
    "                f\"val_loss={val_loss:.4f} acc={val_acc:.4f}\"\n",
    "            )\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    # ---- Test ----\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_examples = 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for Xb, yb in test_loader:\n",
    "            Xb = Xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            logits = model(Xb)\n",
    "            preds = (torch.sigmoid(logits) >= 0.5).float()\n",
    "            test_correct += (preds == yb).float().sum().item()\n",
    "            test_examples += yb.size(0)\n",
    "            \n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_true.append(yb.cpu().numpy())\n",
    "            \n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_true = np.concatenate(all_true)\n",
    "    print(\"Test mean(true):\", all_true.mean())\n",
    "    print(\"Test mean(pred):\", all_preds.mean())\n",
    "\n",
    "    test_acc = test_correct / test_examples if test_examples > 0 else float(\"nan\")\n",
    "    print(f\"[{scale_mode}] Best val acc={best_val_acc:.4f} | Test acc={test_acc:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"model\": \"LSTM\",\n",
    "        \"features\": \"price+macro\",\n",
    "        \"window_size\": window_size,\n",
    "        \"hidden_desc\": f\"hidden={hidden_dim}, layers={num_layers}\",\n",
    "        \"scaled\": use_scaled,\n",
    "        \"best_val_acc\": best_val_acc,\n",
    "        \"test_acc\": test_acc,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e56b717a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "use_scaled = True\n",
      "==============================\n",
      "\n",
      "=== MLP price+macro (scaled) W=30, hidden=(32, 16) ===\n",
      "Train windows: 2214 Val: 727 Test: 722\n",
      "[scaled] Best val acc=0.4223 | Test acc=0.4681\n",
      "{'model': 'MLP', 'features': 'price+macro', 'window_size': 30, 'hidden_desc': '(32, 16)', 'scaled': True, 'best_val_acc': 0.422283356258597, 'test_acc': 0.46814404432132967}\n",
      "\n",
      "=== MLP price+macro (scaled) W=30, hidden=(64, 32) ===\n",
      "Train windows: 2214 Val: 727 Test: 722\n",
      "[scaled] Best val acc=0.4223 | Test acc=0.4681\n",
      "{'model': 'MLP', 'features': 'price+macro', 'window_size': 30, 'hidden_desc': '(64, 32)', 'scaled': True, 'best_val_acc': 0.422283356258597, 'test_acc': 0.46814404432132967}\n",
      "\n",
      "=== MLP price+macro (scaled) W=60, hidden=(64, 32) ===\n",
      "Train windows: 2184 Val: 697 Test: 692\n",
      "[scaled] Best val acc=0.4204 | Test acc=0.4653\n",
      "{'model': 'MLP', 'features': 'price+macro', 'window_size': 60, 'hidden_desc': '(64, 32)', 'scaled': True, 'best_val_acc': 0.42037302725968434, 'test_acc': 0.4653179190751445}\n",
      "\n",
      "=== MLP price+macro (scaled) W=60, hidden=(128, 64) ===\n",
      "Train windows: 2184 Val: 697 Test: 692\n",
      "[scaled] Best val acc=0.4204 | Test acc=0.4653\n",
      "{'model': 'MLP', 'features': 'price+macro', 'window_size': 60, 'hidden_desc': '(128, 64)', 'scaled': True, 'best_val_acc': 0.42037302725968434, 'test_acc': 0.4653179190751445}\n",
      "\n",
      "=== MLP price+macro (scaled) W=90, hidden=(64, 32) ===\n",
      "Train windows: 2154 Val: 667 Test: 662\n",
      "[scaled] Best val acc=0.4198 | Test acc=0.4607\n",
      "{'model': 'MLP', 'features': 'price+macro', 'window_size': 90, 'hidden_desc': '(64, 32)', 'scaled': True, 'best_val_acc': 0.4197901049475262, 'test_acc': 0.4607250755287009}\n",
      "\n",
      "=== GRU price+macro (scaled) W=30, hidden=16, layers=1 ===\n",
      "Train windows: 2214 Val: 727 Test: 722\n",
      "[scaled] Best val acc=0.4223 | Test acc=0.4681\n",
      "{'model': 'GRU', 'features': 'price+macro', 'window_size': 30, 'hidden_desc': 'hidden=16, layers=1', 'scaled': True, 'best_val_acc': 0.422283356258597, 'test_acc': 0.46814404432132967}\n",
      "\n",
      "=== GRU price+macro (scaled) W=30, hidden=32, layers=1 ===\n",
      "Train windows: 2214 Val: 727 Test: 722\n",
      "[scaled] Best val acc=0.4223 | Test acc=0.4681\n",
      "{'model': 'GRU', 'features': 'price+macro', 'window_size': 30, 'hidden_desc': 'hidden=32, layers=1', 'scaled': True, 'best_val_acc': 0.422283356258597, 'test_acc': 0.46814404432132967}\n",
      "\n",
      "=== GRU price+macro (scaled) W=60, hidden=32, layers=1 ===\n",
      "Train windows: 2184 Val: 697 Test: 692\n",
      "[scaled] Best val acc=0.4204 | Test acc=0.4653\n",
      "{'model': 'GRU', 'features': 'price+macro', 'window_size': 60, 'hidden_desc': 'hidden=32, layers=1', 'scaled': True, 'best_val_acc': 0.42037302725968434, 'test_acc': 0.4653179190751445}\n",
      "\n",
      "=== GRU price+macro (scaled) W=60, hidden=64, layers=1 ===\n",
      "Train windows: 2184 Val: 697 Test: 692\n",
      "[scaled] Best val acc=0.4204 | Test acc=0.4653\n",
      "{'model': 'GRU', 'features': 'price+macro', 'window_size': 60, 'hidden_desc': 'hidden=64, layers=1', 'scaled': True, 'best_val_acc': 0.42037302725968434, 'test_acc': 0.4653179190751445}\n",
      "\n",
      "=== GRU price+macro (scaled) W=90, hidden=32, layers=2 ===\n",
      "Train windows: 2154 Val: 667 Test: 662\n",
      "[scaled] Best val acc=0.4198 | Test acc=0.4607\n",
      "{'model': 'GRU', 'features': 'price+macro', 'window_size': 90, 'hidden_desc': 'hidden=32, layers=2', 'scaled': True, 'best_val_acc': 0.4197901049475262, 'test_acc': 0.4607250755287009}\n",
      "\n",
      "=== LSTM price+macro (scaled) W=30, hidden=32, layers=1 ===\n",
      "Train windows: 2214 Val: 727 Test: 722\n",
      "Test mean(true): 0.53185594\n",
      "Test mean(pred): 0.0\n",
      "[scaled] Best val acc=0.4223 | Test acc=0.4681\n",
      "{'model': 'LSTM', 'features': 'price+macro', 'window_size': 30, 'hidden_desc': 'hidden=32, layers=1', 'scaled': True, 'best_val_acc': 0.422283356258597, 'test_acc': 0.46814404432132967}\n",
      "\n",
      "=== LSTM price+macro (scaled) W=60, hidden=32, layers=1 ===\n",
      "Train windows: 2184 Val: 697 Test: 692\n",
      "Test mean(true): 0.5346821\n",
      "Test mean(pred): 0.0\n",
      "[scaled] Best val acc=0.4204 | Test acc=0.4653\n",
      "{'model': 'LSTM', 'features': 'price+macro', 'window_size': 60, 'hidden_desc': 'hidden=32, layers=1', 'scaled': True, 'best_val_acc': 0.42037302725968434, 'test_acc': 0.4653179190751445}\n",
      "\n",
      "=== LSTM price+macro (scaled) W=60, hidden=64, layers=1 ===\n",
      "Train windows: 2184 Val: 697 Test: 692\n",
      "Test mean(true): 0.5346821\n",
      "Test mean(pred): 0.0\n",
      "[scaled] Best val acc=0.4204 | Test acc=0.4653\n",
      "{'model': 'LSTM', 'features': 'price+macro', 'window_size': 60, 'hidden_desc': 'hidden=64, layers=1', 'scaled': True, 'best_val_acc': 0.42037302725968434, 'test_acc': 0.4653179190751445}\n",
      "\n",
      "==============================\n",
      "use_scaled = False\n",
      "==============================\n",
      "\n",
      "=== MLP price+macro (raw) W=30, hidden=(32, 16) ===\n",
      "Train windows: 2214 Val: 727 Test: 722\n",
      "[raw] Best val acc=0.4223 | Test acc=0.4681\n",
      "{'model': 'MLP', 'features': 'price+macro', 'window_size': 30, 'hidden_desc': '(32, 16)', 'scaled': False, 'best_val_acc': 0.422283356258597, 'test_acc': 0.46814404432132967}\n",
      "\n",
      "=== MLP price+macro (raw) W=30, hidden=(64, 32) ===\n",
      "Train windows: 2214 Val: 727 Test: 722\n",
      "[raw] Best val acc=0.4223 | Test acc=0.4681\n",
      "{'model': 'MLP', 'features': 'price+macro', 'window_size': 30, 'hidden_desc': '(64, 32)', 'scaled': False, 'best_val_acc': 0.422283356258597, 'test_acc': 0.46814404432132967}\n",
      "\n",
      "=== MLP price+macro (raw) W=60, hidden=(64, 32) ===\n",
      "Train windows: 2184 Val: 697 Test: 692\n",
      "[raw] Best val acc=0.4204 | Test acc=0.4653\n",
      "{'model': 'MLP', 'features': 'price+macro', 'window_size': 60, 'hidden_desc': '(64, 32)', 'scaled': False, 'best_val_acc': 0.42037302725968434, 'test_acc': 0.4653179190751445}\n",
      "\n",
      "=== MLP price+macro (raw) W=60, hidden=(128, 64) ===\n",
      "Train windows: 2184 Val: 697 Test: 692\n",
      "[raw] Best val acc=0.4204 | Test acc=0.4653\n",
      "{'model': 'MLP', 'features': 'price+macro', 'window_size': 60, 'hidden_desc': '(128, 64)', 'scaled': False, 'best_val_acc': 0.42037302725968434, 'test_acc': 0.4653179190751445}\n",
      "\n",
      "=== MLP price+macro (raw) W=90, hidden=(64, 32) ===\n",
      "Train windows: 2154 Val: 667 Test: 662\n",
      "[raw] Best val acc=0.4198 | Test acc=0.4607\n",
      "{'model': 'MLP', 'features': 'price+macro', 'window_size': 90, 'hidden_desc': '(64, 32)', 'scaled': False, 'best_val_acc': 0.4197901049475262, 'test_acc': 0.4607250755287009}\n",
      "\n",
      "=== GRU price+macro (raw) W=30, hidden=16, layers=1 ===\n",
      "Train windows: 2214 Val: 727 Test: 722\n",
      "[raw] Best val acc=0.4223 | Test acc=0.4681\n",
      "{'model': 'GRU', 'features': 'price+macro', 'window_size': 30, 'hidden_desc': 'hidden=16, layers=1', 'scaled': False, 'best_val_acc': 0.422283356258597, 'test_acc': 0.46814404432132967}\n",
      "\n",
      "=== GRU price+macro (raw) W=30, hidden=32, layers=1 ===\n",
      "Train windows: 2214 Val: 727 Test: 722\n",
      "[raw] Best val acc=0.4223 | Test acc=0.4681\n",
      "{'model': 'GRU', 'features': 'price+macro', 'window_size': 30, 'hidden_desc': 'hidden=32, layers=1', 'scaled': False, 'best_val_acc': 0.422283356258597, 'test_acc': 0.46814404432132967}\n",
      "\n",
      "=== GRU price+macro (raw) W=60, hidden=32, layers=1 ===\n",
      "Train windows: 2184 Val: 697 Test: 692\n",
      "[raw] Best val acc=0.4204 | Test acc=0.4653\n",
      "{'model': 'GRU', 'features': 'price+macro', 'window_size': 60, 'hidden_desc': 'hidden=32, layers=1', 'scaled': False, 'best_val_acc': 0.42037302725968434, 'test_acc': 0.4653179190751445}\n",
      "\n",
      "=== GRU price+macro (raw) W=60, hidden=64, layers=1 ===\n",
      "Train windows: 2184 Val: 697 Test: 692\n",
      "[raw] Best val acc=0.4204 | Test acc=0.4653\n",
      "{'model': 'GRU', 'features': 'price+macro', 'window_size': 60, 'hidden_desc': 'hidden=64, layers=1', 'scaled': False, 'best_val_acc': 0.42037302725968434, 'test_acc': 0.4653179190751445}\n",
      "\n",
      "=== GRU price+macro (raw) W=90, hidden=32, layers=2 ===\n",
      "Train windows: 2154 Val: 667 Test: 662\n",
      "[raw] Best val acc=0.4198 | Test acc=0.4607\n",
      "{'model': 'GRU', 'features': 'price+macro', 'window_size': 90, 'hidden_desc': 'hidden=32, layers=2', 'scaled': False, 'best_val_acc': 0.4197901049475262, 'test_acc': 0.4607250755287009}\n",
      "\n",
      "=== LSTM price+macro (raw) W=30, hidden=32, layers=1 ===\n",
      "Train windows: 2214 Val: 727 Test: 722\n",
      "Test mean(true): 0.53185594\n",
      "Test mean(pred): 0.0\n",
      "[raw] Best val acc=0.4223 | Test acc=0.4681\n",
      "{'model': 'LSTM', 'features': 'price+macro', 'window_size': 30, 'hidden_desc': 'hidden=32, layers=1', 'scaled': False, 'best_val_acc': 0.422283356258597, 'test_acc': 0.46814404432132967}\n",
      "\n",
      "=== LSTM price+macro (raw) W=60, hidden=32, layers=1 ===\n",
      "Train windows: 2184 Val: 697 Test: 692\n",
      "Test mean(true): 0.5346821\n",
      "Test mean(pred): 0.0\n",
      "[raw] Best val acc=0.4204 | Test acc=0.4653\n",
      "{'model': 'LSTM', 'features': 'price+macro', 'window_size': 60, 'hidden_desc': 'hidden=32, layers=1', 'scaled': False, 'best_val_acc': 0.42037302725968434, 'test_acc': 0.4653179190751445}\n",
      "\n",
      "=== LSTM price+macro (raw) W=60, hidden=64, layers=1 ===\n",
      "Train windows: 2184 Val: 697 Test: 692\n",
      "Test mean(true): 0.5346821\n",
      "Test mean(pred): 0.0\n",
      "[raw] Best val acc=0.4204 | Test acc=0.4653\n",
      "{'model': 'LSTM', 'features': 'price+macro', 'window_size': 60, 'hidden_desc': 'hidden=64, layers=1', 'scaled': False, 'best_val_acc': 0.42037302725968434, 'test_acc': 0.4653179190751445}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>features</th>\n",
       "      <th>window_size</th>\n",
       "      <th>hidden_desc</th>\n",
       "      <th>scaled</th>\n",
       "      <th>best_val_acc</th>\n",
       "      <th>test_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP</td>\n",
       "      <td>price+macro</td>\n",
       "      <td>30</td>\n",
       "      <td>(32, 16)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.422283</td>\n",
       "      <td>0.468144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLP</td>\n",
       "      <td>price+macro</td>\n",
       "      <td>30</td>\n",
       "      <td>(64, 32)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.422283</td>\n",
       "      <td>0.468144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP</td>\n",
       "      <td>price+macro</td>\n",
       "      <td>60</td>\n",
       "      <td>(64, 32)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.420373</td>\n",
       "      <td>0.465318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP</td>\n",
       "      <td>price+macro</td>\n",
       "      <td>60</td>\n",
       "      <td>(128, 64)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.420373</td>\n",
       "      <td>0.465318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MLP</td>\n",
       "      <td>price+macro</td>\n",
       "      <td>90</td>\n",
       "      <td>(64, 32)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.419790</td>\n",
       "      <td>0.460725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GRU</td>\n",
       "      <td>price+macro</td>\n",
       "      <td>30</td>\n",
       "      <td>hidden=16, layers=1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.422283</td>\n",
       "      <td>0.468144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GRU</td>\n",
       "      <td>price+macro</td>\n",
       "      <td>30</td>\n",
       "      <td>hidden=32, layers=1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.422283</td>\n",
       "      <td>0.468144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GRU</td>\n",
       "      <td>price+macro</td>\n",
       "      <td>60</td>\n",
       "      <td>hidden=32, layers=1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.420373</td>\n",
       "      <td>0.465318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GRU</td>\n",
       "      <td>price+macro</td>\n",
       "      <td>60</td>\n",
       "      <td>hidden=64, layers=1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.420373</td>\n",
       "      <td>0.465318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GRU</td>\n",
       "      <td>price+macro</td>\n",
       "      <td>90</td>\n",
       "      <td>hidden=32, layers=2</td>\n",
       "      <td>True</td>\n",
       "      <td>0.419790</td>\n",
       "      <td>0.460725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>price+macro</td>\n",
       "      <td>30</td>\n",
       "      <td>hidden=32, layers=1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.422283</td>\n",
       "      <td>0.468144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>price+macro</td>\n",
       "      <td>60</td>\n",
       "      <td>hidden=32, layers=1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.420373</td>\n",
       "      <td>0.465318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>price+macro</td>\n",
       "      <td>60</td>\n",
       "      <td>hidden=64, layers=1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.420373</td>\n",
       "      <td>0.465318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MLP</td>\n",
       "      <td>price+macro</td>\n",
       "      <td>30</td>\n",
       "      <td>(32, 16)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.422283</td>\n",
       "      <td>0.468144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MLP</td>\n",
       "      <td>price+macro</td>\n",
       "      <td>30</td>\n",
       "      <td>(64, 32)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.422283</td>\n",
       "      <td>0.468144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MLP</td>\n",
       "      <td>price+macro</td>\n",
       "      <td>60</td>\n",
       "      <td>(64, 32)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.420373</td>\n",
       "      <td>0.465318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>MLP</td>\n",
       "      <td>price+macro</td>\n",
       "      <td>60</td>\n",
       "      <td>(128, 64)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.420373</td>\n",
       "      <td>0.465318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MLP</td>\n",
       "      <td>price+macro</td>\n",
       "      <td>90</td>\n",
       "      <td>(64, 32)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.419790</td>\n",
       "      <td>0.460725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>GRU</td>\n",
       "      <td>price+macro</td>\n",
       "      <td>30</td>\n",
       "      <td>hidden=16, layers=1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.422283</td>\n",
       "      <td>0.468144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>GRU</td>\n",
       "      <td>price+macro</td>\n",
       "      <td>30</td>\n",
       "      <td>hidden=32, layers=1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.422283</td>\n",
       "      <td>0.468144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>GRU</td>\n",
       "      <td>price+macro</td>\n",
       "      <td>60</td>\n",
       "      <td>hidden=32, layers=1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.420373</td>\n",
       "      <td>0.465318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>GRU</td>\n",
       "      <td>price+macro</td>\n",
       "      <td>60</td>\n",
       "      <td>hidden=64, layers=1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.420373</td>\n",
       "      <td>0.465318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>GRU</td>\n",
       "      <td>price+macro</td>\n",
       "      <td>90</td>\n",
       "      <td>hidden=32, layers=2</td>\n",
       "      <td>False</td>\n",
       "      <td>0.419790</td>\n",
       "      <td>0.460725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>price+macro</td>\n",
       "      <td>30</td>\n",
       "      <td>hidden=32, layers=1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.422283</td>\n",
       "      <td>0.468144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>price+macro</td>\n",
       "      <td>60</td>\n",
       "      <td>hidden=32, layers=1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.420373</td>\n",
       "      <td>0.465318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>price+macro</td>\n",
       "      <td>60</td>\n",
       "      <td>hidden=64, layers=1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.420373</td>\n",
       "      <td>0.465318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model     features  window_size          hidden_desc  scaled  best_val_acc  \\\n",
       "0    MLP  price+macro           30             (32, 16)    True      0.422283   \n",
       "1    MLP  price+macro           30             (64, 32)    True      0.422283   \n",
       "2    MLP  price+macro           60             (64, 32)    True      0.420373   \n",
       "3    MLP  price+macro           60            (128, 64)    True      0.420373   \n",
       "4    MLP  price+macro           90             (64, 32)    True      0.419790   \n",
       "5    GRU  price+macro           30  hidden=16, layers=1    True      0.422283   \n",
       "6    GRU  price+macro           30  hidden=32, layers=1    True      0.422283   \n",
       "7    GRU  price+macro           60  hidden=32, layers=1    True      0.420373   \n",
       "8    GRU  price+macro           60  hidden=64, layers=1    True      0.420373   \n",
       "9    GRU  price+macro           90  hidden=32, layers=2    True      0.419790   \n",
       "10  LSTM  price+macro           30  hidden=32, layers=1    True      0.422283   \n",
       "11  LSTM  price+macro           60  hidden=32, layers=1    True      0.420373   \n",
       "12  LSTM  price+macro           60  hidden=64, layers=1    True      0.420373   \n",
       "13   MLP  price+macro           30             (32, 16)   False      0.422283   \n",
       "14   MLP  price+macro           30             (64, 32)   False      0.422283   \n",
       "15   MLP  price+macro           60             (64, 32)   False      0.420373   \n",
       "16   MLP  price+macro           60            (128, 64)   False      0.420373   \n",
       "17   MLP  price+macro           90             (64, 32)   False      0.419790   \n",
       "18   GRU  price+macro           30  hidden=16, layers=1   False      0.422283   \n",
       "19   GRU  price+macro           30  hidden=32, layers=1   False      0.422283   \n",
       "20   GRU  price+macro           60  hidden=32, layers=1   False      0.420373   \n",
       "21   GRU  price+macro           60  hidden=64, layers=1   False      0.420373   \n",
       "22   GRU  price+macro           90  hidden=32, layers=2   False      0.419790   \n",
       "23  LSTM  price+macro           30  hidden=32, layers=1   False      0.422283   \n",
       "24  LSTM  price+macro           60  hidden=32, layers=1   False      0.420373   \n",
       "25  LSTM  price+macro           60  hidden=64, layers=1   False      0.420373   \n",
       "\n",
       "    test_acc  \n",
       "0   0.468144  \n",
       "1   0.468144  \n",
       "2   0.465318  \n",
       "3   0.465318  \n",
       "4   0.460725  \n",
       "5   0.468144  \n",
       "6   0.468144  \n",
       "7   0.465318  \n",
       "8   0.465318  \n",
       "9   0.460725  \n",
       "10  0.468144  \n",
       "11  0.465318  \n",
       "12  0.465318  \n",
       "13  0.468144  \n",
       "14  0.468144  \n",
       "15  0.465318  \n",
       "16  0.465318  \n",
       "17  0.460725  \n",
       "18  0.468144  \n",
       "19  0.468144  \n",
       "20  0.465318  \n",
       "21  0.465318  \n",
       "22  0.460725  \n",
       "23  0.468144  \n",
       "24  0.465318  \n",
       "25  0.465318  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "mlp_configs = [\n",
    "    {\"window_size\": 30, \"hidden_sizes\": (32, 16)},\n",
    "    {\"window_size\": 30, \"hidden_sizes\": (64, 32)},\n",
    "    {\"window_size\": 60, \"hidden_sizes\": (64, 32)},\n",
    "    {\"window_size\": 60, \"hidden_sizes\": (128, 64)},\n",
    "    {\"window_size\": 90, \"hidden_sizes\": (64, 32)},\n",
    "]\n",
    "\n",
    "gru_configs = [\n",
    "    {\"window_size\": 30, \"hidden_dim\": 16, \"num_layers\": 1},\n",
    "    {\"window_size\": 30, \"hidden_dim\": 32, \"num_layers\": 1},\n",
    "    {\"window_size\": 60, \"hidden_dim\": 32, \"num_layers\": 1},\n",
    "    {\"window_size\": 60, \"hidden_dim\": 64, \"num_layers\": 1},\n",
    "    {\"window_size\": 90, \"hidden_dim\": 32, \"num_layers\": 2},\n",
    "]\n",
    "\n",
    "lstm_configs = [\n",
    "    {\"window_size\": 30, \"hidden_dim\": 32, \"num_layers\": 1},\n",
    "    {\"window_size\": 60, \"hidden_dim\": 32, \"num_layers\": 1},\n",
    "    {\"window_size\": 60, \"hidden_dim\": 64, \"num_layers\": 1},\n",
    "]\n",
    "results = []\n",
    "\n",
    "for use_scaled in [True, False]:\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"use_scaled =\", use_scaled)\n",
    "    print(\"==============================\")\n",
    "\n",
    "    # --- MLPs ---\n",
    "    for cfg in mlp_configs:\n",
    "        res = train_mlp_macro(\n",
    "            window_size=cfg[\"window_size\"],\n",
    "            hidden_sizes=cfg[\"hidden_sizes\"],\n",
    "            batch_size=64,\n",
    "            num_epochs=20,\n",
    "            lr=1e-3,\n",
    "            use_scaled=use_scaled,\n",
    "            verbose=False,\n",
    "        )\n",
    "        print(res)\n",
    "        results.append(res)\n",
    "\n",
    "    # --- GRUs ---\n",
    "    for cfg in gru_configs:\n",
    "        res = train_gru_macro(\n",
    "            window_size=cfg[\"window_size\"],\n",
    "            hidden_dim=cfg[\"hidden_dim\"],\n",
    "            num_layers=cfg[\"num_layers\"],\n",
    "            batch_size=64,\n",
    "            num_epochs=20,\n",
    "            lr=1e-3,\n",
    "            use_scaled=use_scaled,\n",
    "            verbose=False,\n",
    "        )\n",
    "        print(res)\n",
    "        results.append(res)\n",
    "\n",
    "    # --- LSTMs ---\n",
    "    for cfg in lstm_configs:\n",
    "        res = train_lstm_macro(\n",
    "            window_size=cfg[\"window_size\"],\n",
    "            hidden_dim=cfg[\"hidden_dim\"],\n",
    "            num_layers=cfg[\"num_layers\"],\n",
    "            batch_size=64,\n",
    "            num_epochs=20,\n",
    "            lr=1e-3,\n",
    "            use_scaled=use_scaled,\n",
    "            verbose=False,\n",
    "        )\n",
    "        print(res)\n",
    "        results.append(res)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "93b0c128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label_up mean: 0.5508021390374331\n",
      "Val   label_up mean: 0.583883751651255\n",
      "Test  label_up mean: 0.526595744680851\n"
     ]
    }
   ],
   "source": [
    "print(\"Train label_up mean:\", dfm.loc[train_mask, \"label_up\"].mean())\n",
    "print(\"Val   label_up mean:\", dfm.loc[val_mask, \"label_up\"].mean())\n",
    "print(\"Test  label_up mean:\", dfm.loc[test_mask, \"label_up\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "786183d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m all_true = []\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m Xb, yb \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtest_loader\u001b[49m:\n\u001b[32m      7\u001b[39m         Xb = Xb.to(device)\n\u001b[32m      8\u001b[39m         yb = yb.to(device)\n",
      "\u001b[31mNameError\u001b[39m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# After training one of the models:\n",
    "all_preds = []\n",
    "all_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for Xb, yb in test_loader:\n",
    "        Xb = Xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        logits = model(Xb)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs >= 0.5).float()\n",
    "\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_true.append(yb.cpu().numpy())\n",
    "\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_true = np.concatenate(all_true)\n",
    "\n",
    "print(\"Test mean(true):\", all_true.mean())\n",
    "print(\"Test mean(pred):\", all_preds.mean())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
