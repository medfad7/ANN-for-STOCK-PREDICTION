{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51087a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "ROOT = Path(\"..\").resolve()\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT))\n",
    "\n",
    "from src.data.dataset import make_time_splits\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e9c15f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: C:\\Users\\KDP only\\Documents\\ANN_Final_Project\\spy-ann\n",
      "DATA_PATH exists: True\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ROOT = Path.cwd().resolve().parent  # adjust if needed\n",
    "\n",
    "DATA_PATH = PROJECT_ROOT / \"data\" / \"processed\" / \"daily_merged.parquet\"\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"DATA_PATH exists:\", DATA_PATH.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c4eae58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA PREVIEW ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "      <th>ret_1d</th>\n",
       "      <th>log_ret_1d</th>\n",
       "      <th>ma_close_5</th>\n",
       "      <th>ma_close_20</th>\n",
       "      <th>vol_5</th>\n",
       "      <th>vol_20</th>\n",
       "      <th>future_price</th>\n",
       "      <th>future_ret_1d</th>\n",
       "      <th>label_up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-02-02</td>\n",
       "      <td>83.059364</td>\n",
       "      <td>83.217386</td>\n",
       "      <td>81.930636</td>\n",
       "      <td>82.216584</td>\n",
       "      <td>216327900</td>\n",
       "      <td>0.012104</td>\n",
       "      <td>0.012031</td>\n",
       "      <td>82.055548</td>\n",
       "      <td>84.347997</td>\n",
       "      <td>0.012653</td>\n",
       "      <td>0.010585</td>\n",
       "      <td>82.645493</td>\n",
       "      <td>-0.004983</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-02-03</td>\n",
       "      <td>82.645493</td>\n",
       "      <td>83.134609</td>\n",
       "      <td>82.404697</td>\n",
       "      <td>82.683113</td>\n",
       "      <td>172730700</td>\n",
       "      <td>-0.004983</td>\n",
       "      <td>-0.004995</td>\n",
       "      <td>82.055548</td>\n",
       "      <td>84.205024</td>\n",
       "      <td>0.012873</td>\n",
       "      <td>0.010574</td>\n",
       "      <td>80.094604</td>\n",
       "      <td>-0.030865</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-02-04</td>\n",
       "      <td>80.094604</td>\n",
       "      <td>82.043540</td>\n",
       "      <td>80.079552</td>\n",
       "      <td>82.005919</td>\n",
       "      <td>356715700</td>\n",
       "      <td>-0.030865</td>\n",
       "      <td>-0.031352</td>\n",
       "      <td>81.734995</td>\n",
       "      <td>83.931498</td>\n",
       "      <td>0.018783</td>\n",
       "      <td>0.012403</td>\n",
       "      <td>80.260124</td>\n",
       "      <td>0.002067</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>80.260124</td>\n",
       "      <td>80.425666</td>\n",
       "      <td>78.694953</td>\n",
       "      <td>80.184871</td>\n",
       "      <td>493585800</td>\n",
       "      <td>0.002067</td>\n",
       "      <td>0.002064</td>\n",
       "      <td>81.625131</td>\n",
       "      <td>83.648186</td>\n",
       "      <td>0.018457</td>\n",
       "      <td>0.012344</td>\n",
       "      <td>79.680710</td>\n",
       "      <td>-0.007219</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-02-08</td>\n",
       "      <td>79.680710</td>\n",
       "      <td>80.764291</td>\n",
       "      <td>79.620510</td>\n",
       "      <td>80.320322</td>\n",
       "      <td>224166900</td>\n",
       "      <td>-0.007219</td>\n",
       "      <td>-0.007245</td>\n",
       "      <td>81.148059</td>\n",
       "      <td>83.321606</td>\n",
       "      <td>0.015917</td>\n",
       "      <td>0.012270</td>\n",
       "      <td>80.681526</td>\n",
       "      <td>0.012560</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date      Close       High        Low       Open     Volume    ret_1d  \\\n",
       "0 2010-02-02  83.059364  83.217386  81.930636  82.216584  216327900  0.012104   \n",
       "1 2010-02-03  82.645493  83.134609  82.404697  82.683113  172730700 -0.004983   \n",
       "2 2010-02-04  80.094604  82.043540  80.079552  82.005919  356715700 -0.030865   \n",
       "3 2010-02-05  80.260124  80.425666  78.694953  80.184871  493585800  0.002067   \n",
       "4 2010-02-08  79.680710  80.764291  79.620510  80.320322  224166900 -0.007219   \n",
       "\n",
       "   log_ret_1d  ma_close_5  ma_close_20     vol_5    vol_20  future_price  \\\n",
       "0    0.012031   82.055548    84.347997  0.012653  0.010585     82.645493   \n",
       "1   -0.004995   82.055548    84.205024  0.012873  0.010574     80.094604   \n",
       "2   -0.031352   81.734995    83.931498  0.018783  0.012403     80.260124   \n",
       "3    0.002064   81.625131    83.648186  0.018457  0.012344     79.680710   \n",
       "4   -0.007245   81.148059    83.321606  0.015917  0.012270     80.681526   \n",
       "\n",
       "   future_ret_1d  label_up  \n",
       "0      -0.004983         0  \n",
       "1      -0.030865         0  \n",
       "2       0.002067         1  \n",
       "3      -0.007219         0  \n",
       "4       0.012560         1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape: (3753, 15)\n",
      "Date range: 2010-02-02 00:00:00 → 2024-12-30 00:00:00\n",
      "\n",
      "Columns: ['date', 'Close', 'High', 'Low', 'Open', 'Volume', 'ret_1d', 'log_ret_1d', 'ma_close_5', 'ma_close_20', 'vol_5', 'vol_20', 'future_price', 'future_ret_1d', 'label_up']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(DATA_PATH)\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "print(\"=== DATA PREVIEW ===\")\n",
    "display(df.head())\n",
    "print(\"\\nShape:\", df.shape)\n",
    "print(\"Date range:\", df[\"date\"].min(), \"→\", df[\"date\"].max())\n",
    "\n",
    "print(\"\\nColumns:\", df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06cf149e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature columns: ['Close', 'High', 'Low', 'Open', 'Volume', 'ret_1d', 'log_ret_1d', 'ma_close_5', 'ma_close_20', 'vol_5', 'vol_20']\n",
      "Num features: 11\n",
      "X_all shape: (3753, 11)\n",
      "y_all shape: (3753,)\n",
      "y_all stats: mean=0.000576, std=0.010741\n"
     ]
    }
   ],
   "source": [
    "target_col = \"future_ret_1d\"\n",
    "\n",
    "drop_cols = [\"date\", \"future_price\", \"label_up\", target_col]\n",
    "feature_cols = [c for c in df.columns if c not in drop_cols]\n",
    "\n",
    "print(\"\\nFeature columns:\", feature_cols)\n",
    "print(\"Num features:\", len(feature_cols))\n",
    "\n",
    "X_all = df[feature_cols].values.astype(\"float32\")\n",
    "y_all = df[target_col].values.astype(\"float32\")\n",
    "\n",
    "print(\"X_all shape:\", X_all.shape)\n",
    "print(\"y_all shape:\", y_all.shape)\n",
    "print(\"y_all stats: mean={:.6f}, std={:.6f}\".format(y_all.mean(), y_all.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01b41030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 2244\n",
      "Val size: 757\n",
      "Test size: 752\n"
     ]
    }
   ],
   "source": [
    "train_end = \"2018-12-31\"\n",
    "val_end = \"2021-12-31\"\n",
    "\n",
    "splits = make_time_splits(df, train_end=train_end, val_end=val_end)\n",
    "\n",
    "train_idx = splits.train_idx\n",
    "val_idx = splits.val_idx\n",
    "test_idx = splits.test_idx\n",
    "\n",
    "print(\"Train size:\", len(train_idx))\n",
    "print(\"Val size:\", len(val_idx))\n",
    "print(\"Test size:\", len(test_idx))\n",
    "\n",
    "X_train, X_val, X_test = X_all[train_idx], X_all[val_idx], X_all[test_idx]\n",
    "y_train, y_val, y_test = y_all[train_idx], y_all[val_idx], y_all[test_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca56eab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2244, 11), (757, 11), (752, 11))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_val_s   = scaler.transform(X_val)\n",
    "X_test_s  = scaler.transform(X_test)\n",
    "\n",
    "X_train_s.shape, X_val_s.shape, X_test_s.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20841c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train_loss=0.001006 | val_loss=0.000537\n",
      "Epoch 10 | train_loss=0.000094 | val_loss=0.000305\n",
      "Epoch 20 | train_loss=0.000088 | val_loss=0.000481\n",
      "Epoch 30 | train_loss=0.000091 | val_loss=0.000252\n",
      "Epoch 40 | train_loss=0.000086 | val_loss=0.000192\n",
      "Epoch 50 | train_loss=0.000087 | val_loss=0.000186\n",
      "\n",
      "=== MLP Regression Test Metrics ===\n",
      "MSE: 0.00013306\n",
      "MAE: 0.00885392\n",
      "Corr(y_true, y_pred): -0.0371\n",
      "Directional accuracy (sign(pred) vs sign(true)): 0.4681\n",
      "Mean(true): 0.000391, Mean(pred): -0.002280\n"
     ]
    }
   ],
   "source": [
    "class MLPRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=(64, 32)):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = input_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, 1))  # scalar output\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return out.squeeze(-1)  # (batch,)\n",
    "    \n",
    "    \n",
    "    def train_mlp_reg(\n",
    "        Xtr, ytr,\n",
    "        Xval, yval,\n",
    "        Xte, yte,\n",
    "        hidden_dims=(64, 32),\n",
    "        num_epochs=50,\n",
    "        batch_size=64,\n",
    "        lr=1e-3,\n",
    "        weight_decay=0.0,\n",
    "        verbose=True,\n",
    "    ):\n",
    "        input_dim = Xtr.shape[1]\n",
    "        model = MLPRegressor(input_dim, hidden_dims).to(device)\n",
    "\n",
    "        train_ds = TensorDataset(\n",
    "            torch.from_numpy(Xtr).float(),\n",
    "            torch.from_numpy(ytr).float()\n",
    "        )\n",
    "        val_ds = TensorDataset(\n",
    "            torch.from_numpy(Xval).float(),\n",
    "            torch.from_numpy(yval).float()\n",
    "        )\n",
    "        test_ds = TensorDataset(\n",
    "            torch.from_numpy(Xte).float(),\n",
    "            torch.from_numpy(yte).float()\n",
    "        )\n",
    "\n",
    "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "        val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "        test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        optim = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        best_val_loss = float(\"inf\")\n",
    "        best_state = None\n",
    "\n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            # ----- train -----\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            n = 0\n",
    "            for xb, yb in train_loader:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "\n",
    "                optim.zero_grad()\n",
    "                preds = model(xb)\n",
    "                loss = criterion(preds, yb)\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "\n",
    "                total_loss += loss.item() * yb.size(0)\n",
    "                n += yb.size(0)\n",
    "            train_loss = total_loss / n\n",
    "\n",
    "            # ----- val -----\n",
    "            model.eval()\n",
    "            total_loss_val = 0.0\n",
    "            n_val = 0\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in val_loader:\n",
    "                    xb = xb.to(device)\n",
    "                    yb = yb.to(device)\n",
    "                    preds = model(xb)\n",
    "                    loss = criterion(preds, yb)\n",
    "                    total_loss_val += loss.item() * yb.size(0)\n",
    "                    n_val += yb.size(0)\n",
    "            val_loss = total_loss_val / n_val\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_state = model.state_dict()\n",
    "\n",
    "            if verbose and (epoch % 10 == 0 or epoch == 1):\n",
    "                print(f\"Epoch {epoch:02d} | train_loss={train_loss:.6f} | val_loss={val_loss:.6f}\")\n",
    "\n",
    "        if best_state is not None:\n",
    "            model.load_state_dict(best_state)\n",
    "\n",
    "        # ----- test metrics -----\n",
    "        model.eval()\n",
    "        preds_list = []\n",
    "        y_list = []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in test_loader:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "                preds = model(xb)\n",
    "                preds_list.append(preds.cpu().numpy())\n",
    "                y_list.append(yb.cpu().numpy())\n",
    "\n",
    "        y_true = np.concatenate(y_list)\n",
    "        y_pred = np.concatenate(preds_list)\n",
    "\n",
    "        mse = np.mean((y_pred - y_true) ** 2)\n",
    "        mae = np.mean(np.abs(y_pred - y_true))\n",
    "        corr = pearsonr(y_true, y_pred)[0] if y_true.std() > 0 and y_pred.std() > 0 else np.nan\n",
    "\n",
    "        # directional accuracy\n",
    "        dir_true = (y_true > 0).astype(int)\n",
    "        dir_pred = (y_pred > 0).astype(int)\n",
    "        dir_acc = (dir_true == dir_pred).mean()\n",
    "\n",
    "        print(\"\\n=== MLP Regression Test Metrics ===\")\n",
    "        print(f\"MSE: {mse:.8f}\")\n",
    "        print(f\"MAE: {mae:.8f}\")\n",
    "        print(f\"Corr(y_true, y_pred): {corr:.4f}\")\n",
    "        print(f\"Directional accuracy (sign(pred) vs sign(true)): {dir_acc:.4f}\")\n",
    "        print(f\"Mean(true): {y_true.mean():.6f}, Mean(pred): {y_pred.mean():.6f}\")\n",
    "\n",
    "        return model, (mse, mae, corr, dir_acc)\n",
    "    \n",
    "    mlp_model, mlp_metrics = train_mlp_reg(\n",
    "    X_train_s, y_train,\n",
    "    X_val_s, y_val,\n",
    "    X_test_s, y_test,\n",
    "    hidden_dims=(64, 32),\n",
    "    num_epochs=50,\n",
    "    batch_size=64,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    verbose=True,\n",
    "    )\n",
    "    mlp_metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4bdf3cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_windows(X, y, window_size):\n",
    "    X_seq, y_seq = [], []\n",
    "    T = len(X)\n",
    "    for t in range(T - window_size):\n",
    "        X_seq.append(X[t:t+window_size])\n",
    "        y_seq.append(y[t+window_size])\n",
    "    return np.stack(X_seq), np.array(y_seq, dtype=\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9722d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train windows: 2214, Val: 727, Test: 722\n",
      "Epoch 01 | train_loss=0.009207 | val_loss=0.003508\n",
      "Epoch 05 | train_loss=0.000130 | val_loss=0.000802\n",
      "Epoch 10 | train_loss=0.000104 | val_loss=0.000933\n",
      "Epoch 15 | train_loss=0.000099 | val_loss=0.000982\n",
      "Epoch 20 | train_loss=0.000093 | val_loss=0.000694\n",
      "Epoch 25 | train_loss=0.000092 | val_loss=0.000541\n",
      "Epoch 30 | train_loss=0.000096 | val_loss=0.000508\n",
      "\n",
      "=== GRU Regression Test Metrics ===\n",
      "MSE: 0.00232785\n",
      "MAE: 0.03998495\n",
      "Corr(y_true, y_pred): 0.0024\n",
      "Directional accuracy (sign(pred) vs sign(true)): 0.5526\n",
      "Mean(true): 0.000499, Mean(pred): 0.034328\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float32(0.0023278547),\n",
       " np.float32(0.039984953),\n",
       " np.float32(0.0024249603),\n",
       " np.float64(0.5526315789473685))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, X_seq, y_seq):\n",
    "        self.X = torch.from_numpy(X_seq).float()\n",
    "        self.y = torch.from_numpy(y_seq).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "class GRURegressor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=32, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, input_dim)\n",
    "        out, h_n = self.gru(x)\n",
    "        last_hidden = h_n[-1]  # (batch, hidden_dim)\n",
    "        out = self.fc(last_hidden)\n",
    "        return out.squeeze(-1)\n",
    "    \n",
    "def train_gru_reg(\n",
    "    Xtr, ytr,\n",
    "    Xval, yval,\n",
    "    Xte, yte,\n",
    "    window_size=30,\n",
    "    hidden_dim=32,\n",
    "    num_layers=1,\n",
    "    num_epochs=30,\n",
    "    batch_size=64,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    verbose=True,\n",
    "):\n",
    "# build windows separately for train/val/test\n",
    "    Xtr_seq, ytr_seq = build_windows(Xtr, ytr, window_size)\n",
    "    Xval_seq, yval_seq = build_windows(Xval, yval, window_size)\n",
    "    Xte_seq, yte_seq = build_windows(Xte, yte, window_size)\n",
    "\n",
    "    print(f\"Train windows: {Xtr_seq.shape[0]}, Val: {Xval_seq.shape[0]}, Test: {Xte_seq.shape[0]}\")\n",
    "\n",
    "    train_ds = SeqDataset(Xtr_seq, ytr_seq)\n",
    "    val_ds   = SeqDataset(Xval_seq, yval_seq)\n",
    "    test_ds  = SeqDataset(Xte_seq, yte_seq)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    input_dim = Xtr.shape[1]\n",
    "    model = GRURegressor(input_dim, hidden_dim=hidden_dim, num_layers=num_layers).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # ---- train ----\n",
    "        model.train()\n",
    "        tot_loss = 0.0\n",
    "        n = 0\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            optim.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            tot_loss += loss.item() * yb.size(0)\n",
    "            n += yb.size(0)\n",
    "        train_loss = tot_loss / n\n",
    "\n",
    "        # ---- val ----\n",
    "        model.eval()\n",
    "        tot_val = 0.0\n",
    "        n_val = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "                preds = model(xb)\n",
    "                loss = criterion(preds, yb)\n",
    "                tot_val += loss.item() * yb.size(0)\n",
    "                n_val += yb.size(0)\n",
    "        val_loss = tot_val / n_val\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_state = model.state_dict()\n",
    "\n",
    "        if verbose and (epoch % 5 == 0 or epoch == 1):\n",
    "            print(f\"Epoch {epoch:02d} | train_loss={train_loss:.6f} | val_loss={val_loss:.6f}\")\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    # ---- test metrics ----\n",
    "    model.eval()\n",
    "    preds_list, y_list = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            preds = model(xb)\n",
    "            preds_list.append(preds.cpu().numpy())\n",
    "            y_list.append(yb.cpu().numpy())\n",
    "\n",
    "    y_true = np.concatenate(y_list)\n",
    "    y_pred = np.concatenate(preds_list)\n",
    "\n",
    "    mse = np.mean((y_pred - y_true) ** 2)\n",
    "    mae = np.mean(np.abs(y_pred - y_true))\n",
    "    corr = pearsonr(y_true, y_pred)[0] if y_true.std() > 0 and y_pred.std() > 0 else np.nan\n",
    "\n",
    "    dir_true = (y_true > 0).astype(int)\n",
    "    dir_pred = (y_pred > 0).astype(int)\n",
    "    dir_acc = (dir_true == dir_pred).mean()\n",
    "\n",
    "    print(\"\\n=== GRU Regression Test Metrics ===\")\n",
    "    print(f\"MSE: {mse:.8f}\")\n",
    "    print(f\"MAE: {mae:.8f}\")\n",
    "    print(f\"Corr(y_true, y_pred): {corr:.4f}\")\n",
    "    print(f\"Directional accuracy (sign(pred) vs sign(true)): {dir_acc:.4f}\")\n",
    "    print(f\"Mean(true): {y_true.mean():.6f}, Mean(pred): {y_pred.mean():.6f}\")\n",
    "\n",
    "    return model, (mse, mae, corr, dir_acc)\n",
    "\n",
    "gru_model, gru_metrics = train_gru_reg(\n",
    "    X_train_s, y_train,\n",
    "    X_val_s, y_val,\n",
    "    X_test_s, y_test,\n",
    "    window_size=30,\n",
    "    hidden_dim=32,\n",
    "    num_layers=1,\n",
    "    num_epochs=30,\n",
    "    batch_size=64,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    verbose=True,\n",
    ")\n",
    "gru_metrics\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
